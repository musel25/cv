\begin{rubricnorepeat}{Projects}
\noentry{~}

\entry*[Instruction Tuning (SFT+QLoRA)]
End-to-end instruction-tuning workflow with prompt templating, tokenization, and QLoRA adapters; reproducible configs, logged runs, checkpoints, and artifacts; containerized runner.\hfill \href{https://github.com/musel25/instruction-tuning}{\faGithub}

\entry*[Preference Alignment (DPO)]
Pairwise preference data generation and curation; Direct Preference Optimization adapters trained and compared against SFT baselines with deterministic evals and refusal/guardrail checks.\hfill \href{https://github.com/musel25/preference-alignment-dpo}{\faGithub}

\entry*[Instruction Dataset Factory]
Data pipeline for instruction/chat datasets: rule-based filtering, deduplication, decontamination, quality scoring, augmentation, and chat-template rendering; dataset cards + stats.\hfill \href{https://github.com/musel25/instruction-dataset-factory}{\faGithub}

\entry*[LLM Twin Collection Pipeline]
ZenML-orchestrated crawlers (GitHub/Medium/Custom) with dispatcher pattern and retries; raw documents stored in MongoDB via an ODM, with Selenium fallbacks and metadata lineage.\hfill \href{https://github.com/musel25/llm-twin-data-pipeline}{\faGithub}

\entry*[RAG Feature Pipeline]
Cleaning, chunking, and embedding into Qdrant with change-data-capture snapshotting; Pydantic domain entities, OVM dispatchers, and HNSW index configuration for precision/latency tradeoffs.\hfill \href{https://github.com/musel25/rag-feature-pipeline}{\faGithub}

\entry*[Advanced RAG Inference]
Retrieval module with query expansion, self-querying, filtered vector search, and reranking.
% ; pluggable retriever/reranker interfaces and tracing hooks for analysis.
\hfill \href{https://github.com/musel25/rag-inference-advanced}{\faGithub}

\entry*[LLM Evaluation]
Reproducible offline evaluations (general/task/RAG) with deterministic prompts/seeds, contamination checks, and RAG metrics (Ragas/ARES); minimal leaderboard UI and model cards.\hfill \href{https://github.com/musel25/llm-eval-harness}{\faGithub}

\entry*[Inference Quantization]
Throughput/latency benchmarks across GGUF, GPTQ, and EXL2 variants; experiments with KV cache, continuous batching, and speculative decoding where supported; compact report.\hfill \href{https://github.com/musel25/inference-optimization-quantization}{\faGithub}

\entry*[SageMaker Inference]
Hugging Face DLC deployment on AWS SageMaker; FastAPI business service calling the endpoint.
% ; autoscaling targets/policies, client SDK, and request/response logging.
\hfill \href{https://github.com/musel25/sagemaker-inference-microservice}{\faGithub}

\entry*[LLMOps: CI/CD/CT]
GitHub Actions pipelines for CI/CD/CT; Dockerized ZenML pipelines on cloud backends; Comet experiment tracking, Opik prompt monitoring, alerting, and run-time guardrails.\hfill \href{https://github.com/musel25/llmops-cicd-monitoring}{\faGithub}

\entry*[Telemetry RAG YANG]
End-to-end RAG indexing lab telemetry docs (Markdown/YANG/JSON) into Qdrant; context-only prompting via Ollama/OpenAI; reproducible Docker setup and CLI for index/ask.\hfill 
\href{https://github.com/musel25/telemetry-rag}{\faGithub}

\entry*[Medical LV Segmentation]
U-Net for left-ventricle segmentation on Stanford EchoNet-Dynamic; compared pixel-wise masks vs. landmark detection; Eval and morphology processing.\hfill\href{https://github.com/Tec-AI-23/EchoNetDynamic}{\faGithub}

\entry*[ML for Activity Recognition]
Evaluated 18 ML algorithms with grid search on accelerometer data to classify activity levels in older adults; validated via CV and significance tests.\hfill \href{https://github.com/Tec-AI-23/HAR70}{\faGithub}

\entry*[Vision Hydrological Forecasting]
Deep CNN pipeline predicting river stage from imagery; EfficientNet transfer learning and multi-modal inputs (visual + hydrological) for accuracy gains.\hfill \href{https://github.com/musel25/research_deep_learning}{\faGithub}

\entry*[LLM + RAG for Metaheuristics]
Automated framework using RAG + LLMs to generate and refine metaheuristics for black-box optimisation; integrated Qwen2.5-coder, ChromaDB, and Optuna.\hfill \href{https://github.com/musel25/llm-metaheuristics}{\faGithub}

\entry*[Leukocyte CNN + LIME]
CNN to classify leukocytes (cancerous vs non-cancerous) with LIME explanations to highlight decision-driving regions.

\entry*[Fine-tuned GPT-2 Chatbot]
Fine-tuned GPT-2 on WhatsApp chats; regex parsing to HF Datasets; trained and deployed interactive chatbot pipeline.\hfill \href{https://github.com/musel25/her_if_it_was_2025}{\faGithub}

\entry*[NLP Web App]
Real-time text emotion classifier with logging and analytics; built using Streamlit and standard NLP preprocessing.\hfill \href{https://github.com/musel25/nlp_project}{\faGithub}



\end{rubricnorepeat}




% PAST PROJECTS (ARCHIVE)

% \entry*[Instruction Tuning (SFT + QLoRA)]
% End-to-end instruction-tuning workflow with prompt templating, tokenization, and QLoRA adapters; reproducible configs, logged runs, checkpoints, and artifacts; containerized runner.\hfill \href{https://github.com/musel25/instruction-tuning}{\faGithub}

% \entry*[Preference Optimization (DPO/ORPO)]
% Post-training with pairwise preference data; trained/evaluated DPO/ORPO adapters, added guardrails/refusals, and compared pre/post behavior on task-specific evals.\hfill \href{https://github.com/musel25/preference-optimization}{\faGithub}

% \entry*[LLM Evaluation Harness \&  Leaderboard]
% Reproducible offline evals across reasoning/knowledge/safety with deterministic prompts/seeds; compact leaderboard and model cards; contamination checks and error analysis.\hfill \href{https://github.com/musel25/evaluation-harness}{\faGithub}

% \entry*[High Performance Serving(vLLM+ TensorRT)]
% Production-style serving optimized for throughput/latency with quantization and observability; streaming tokens, autoscaling, tracing, and A/B backend switching behind a clean API.\hfill \href{https://github.com/musel25/high-performance-serving}{\faGithub}

% \entry*[Multi-Node Fine-Tuning on Slurm]
% Distributed fine-tuning on HPC with checkpointing, preemption safety, NCCL tuning, shard-aware dataloading, and artifact sync; “zero to cluster” guide.\hfill \href{https://github.com/musel25/slurm-fine-tuning}{\faGithub}

% \entry*[ML Obesity Estimation]
% End-to-end pipeline classification on UCI obesity dataset; DR (PCA, t-SNE), mutual information, 31 models; best: XGBoost (96\%).\hfill \href{https://github.com/musel25/obesity_ml}{\faGithub}

% \entry*[Advanced Statistical Learning]
% From-scratch Monte Carlo, Bootstrap, PCA, multiple regression (Ridge, Lasso), and classic classifiers.
% \hfill 
% \href{https://github.com/musel25/Advanced_Machine_Learning}{\faGithub}

% \entry*[NLP Project Collection]
% Curated models: DistilBERT sentiment (PyTorch), Levenshtein spell checker, and Keras RNN text generator with temperature control.
% \hfill 
% \href{https://github.com/musel25/nlp_projects}{\faGithub}